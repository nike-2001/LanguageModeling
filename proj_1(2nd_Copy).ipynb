{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhaScXLXMssP"
      },
      "source": [
        "## Project 1: Language Modeling\n",
        "\n",
        "In this project, you will implement several different types of language models for text.  We'll start with n-gram models, then move on to neural n-gram and LSTM language models.\n",
        "\n",
        "**Warning: Do not start this project the day before it is due!**\n",
        "Some parts require 20 minutes or more to run, so debugging and tuning can take a significant amount of time.\n",
        "\n",
        "Our dataset for this project will be the WikiText2 language modeling dataset.  We provide some of the basic preprocessing, such as tokenization and rare word filtering (using the `<unk>` token).\n",
        "Therefore, we can assume that all word types in the val/test set appear at least once in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6bqkde0z7f5"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5DAnNX1NAM-"
      },
      "outputs": [],
      "source": [
        "# This block handles some imports and defines some constants.\n",
        "# You shouldn't need to edit this, but if you want to\n",
        "# import other standard python packages, that is fine.\n",
        "\n",
        "# imports\n",
        "from collections import Counter, defaultdict\n",
        "import copy\n",
        "import numpy as np\n",
        "import math\n",
        "import tqdm\n",
        "import random\n",
        "import pdb\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Some constants\n",
        "UNK_TOK = \"<unk>\"\n",
        "PAD_TOK = \"<pad>\"\n",
        "EOS_TOK = \"<eos>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phx4vfmLNB2f"
      },
      "outputs": [],
      "source": [
        "# This block defines the Vocabulary class we need later.\n",
        "# You shouldn't need to edit this.\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, train_text: List[str], min_freq=0):\n",
        "        \"\"\"\n",
        "        We collect counts from train_text.\n",
        "        train_text: a list of tokens.\n",
        "        min_freq: if a token appears strictly less than this, it will not be\n",
        "            added to vocab.\n",
        "        \"\"\"\n",
        "        special_tokens = [UNK_TOK, PAD_TOK, EOS_TOK]\n",
        "\n",
        "        counter = Counter(train_text)\n",
        "        # Note that the order is fixed as long as the training text is the same.\n",
        "        # it's sorted by frequency.\n",
        "        all_tokens = [\n",
        "            t for t, c in counter.most_common()\n",
        "            if c >= min_freq and t not in special_tokens\n",
        "        ]\n",
        "\n",
        "        self.all_tokens = special_tokens + all_tokens\n",
        "        self.str_to_id = {s: i for i, s in enumerate(self.all_tokens)}\n",
        "\n",
        "        self.unk_tok = UNK_TOK\n",
        "        self.pad_tok = PAD_TOK\n",
        "        self.eos_tok = EOS_TOK\n",
        "\n",
        "    def size(self) -> int:\n",
        "        return len(self.all_tokens)\n",
        "\n",
        "\n",
        "    def ids_to_strs(self, indices: List[int]) -> List[str]:\n",
        "        return [self.all_tokens[ii] for ii in indices]\n",
        "\n",
        "\n",
        "    def strs_to_ids(self, strings: List[str]) -> List[int]:\n",
        "        return [self.str_to_id[s] for s in strings]\n",
        "\n",
        "\n",
        "    def __contains__(self, token: str) -> bool:\n",
        "        return token in self.str_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUZz4sErNEi_"
      },
      "outputs": [],
      "source": [
        "# This block downloads and processes the data.\n",
        "# You shouldn't need to edit this.\n",
        "\n",
        "wikitext2_dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
        "print(f\"Raw train examples: {wikitext2_dataset['train']['text'][:10]}\")\n",
        "\n",
        "# just use the simplest one for now\n",
        "tokenizer = lambda x: x.split()\n",
        "\n",
        "# tokenize datatsets\n",
        "def preprocess(_dataset: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Each sentence in _dataset is tokenized into a list of strings.\n",
        "    _dataset: List[str]. Each string is a sentence.\n",
        "    \"\"\"\n",
        "    ret = []\n",
        "    for sent in _dataset:\n",
        "        sent = sent.rstrip('\\n')\n",
        "        # skip empty sentences\n",
        "        if not sent:\n",
        "            continue\n",
        "        # add EOS to the end of sentence\n",
        "        ret += tokenizer(sent) + [EOS_TOK]\n",
        "    return ret\n",
        "\n",
        "tok_train_dataset = preprocess(wikitext2_dataset['train']['text'])\n",
        "tok_validation_dataset = preprocess(wikitext2_dataset['validation']['text'])\n",
        "tok_test_dataset = preprocess(wikitext2_dataset['test']['text'])\n",
        "print(f\"Dataset size (#tokens) - Train: {len(tok_train_dataset)}; Validation: {len(tok_validation_dataset)}; Test: {len(tok_test_dataset)}.\")\n",
        "\n",
        "# build vocabulary: use `min_freq` to model UNK in training\n",
        "### You'll need this vocab throughout this HW.\n",
        "vocab = Vocab(tok_train_dataset, min_freq=2)\n",
        "print(f\"Vocab size: {vocab.size()}. Examples: {vocab.ids_to_strs(list(range(20)))}\")\n",
        "\n",
        "# handle UNKs properly\n",
        "def replace_unseen_with_unk(_dataset: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    We replace the unseen tokens in _dataset with vocab.unk_tok.\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "    for tok in _dataset:\n",
        "        if tok in vocab:\n",
        "            new_data.append(tok)\n",
        "        else:\n",
        "            new_data.append(vocab.unk_tok)\n",
        "    return new_data\n",
        "\n",
        "### You'll need these three datasets throughout this HW.\n",
        "tok_train_dataset = replace_unseen_with_unk(tok_train_dataset)\n",
        "tok_validation_dataset = replace_unseen_with_unk(tok_validation_dataset)\n",
        "tok_test_dataset = replace_unseen_with_unk(tok_test_dataset)\n",
        "print(f\"Final train examples: {tok_train_dataset[:40]}\")\n",
        "print(f\"Final val examples: {tok_validation_dataset[:40]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R73vhp_6NGiT"
      },
      "source": [
        "We've implemented a unigram model here as a demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iszdnjbpNHq2"
      },
      "outputs": [],
      "source": [
        "class UnigramModel:\n",
        "    def __init__(self, train_text: List[str]):\n",
        "        self.counts = Counter(train_text)\n",
        "        self.total_count = len(train_text)\n",
        "\n",
        "\n",
        "    def probability(self, word: str) -> float:\n",
        "        return self.counts[word] / self.total_count\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix: List[str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Return a list of probabilities for each word in the vocabulary.\n",
        "        In unigram model, `text_prefix` doesn't matter as we are not using any\n",
        "            context at all.\n",
        "        \"\"\"\n",
        "        return [self.probability(word) for word in vocab.all_tokens]\n",
        "\n",
        "    def perplexity(self, full_text: List[str]) -> float:\n",
        "        \"\"\"Return the perplexity of the model on a text as a float.\n",
        "\n",
        "        full_text -- a list of string tokens\n",
        "        \"\"\"\n",
        "        log_probabilities = []\n",
        "        for word in full_text:\n",
        "            # Note that the base of the log doesn't matter\n",
        "            # as long as the log and exp use the same base.\n",
        "            log_probabilities.append(math.log(self.probability(word), 2))\n",
        "        return 2 ** -np.mean(log_probabilities)\n",
        "\n",
        "unigram_demonstration_model = UnigramModel(tok_train_dataset)\n",
        "print('unigram validation perplexity:',\n",
        "      unigram_demonstration_model.perplexity(tok_test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5ktxvOSNVcC"
      },
      "outputs": [],
      "source": [
        "def check_validity(model):\n",
        "    \"\"\"\n",
        "    Performs several sanity checks on your model:\n",
        "      1) That `next_word_probabilities` returns a valid distribution\n",
        "      2) That perplexity matches a perplexity calculated from `next_word_probabilities`\n",
        "\n",
        "    Although it is possible to calculate perplexity from `next_word_probabilities`,\n",
        "      it is still good to have a separate more efficient method that only computes\n",
        "      the probabilities of observed words.\n",
        "    \"\"\"\n",
        "\n",
        "    log_probabilities = []\n",
        "    for i in range(10):\n",
        "        prefix = tok_validation_dataset[:i]\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        assert min(probs) >= 0, \"Negative value in next_word_probabilities\"\n",
        "        assert max(probs) <= 1 + 1e-8, \"Value larger than 1 in next_word_probabilities\"\n",
        "        assert abs(sum(probs)-1) < 1e-4, \"next_word_probabilities do not sum to 1\"\n",
        "\n",
        "        word_id = vocab.str_to_id[tok_validation_dataset[i]]\n",
        "        selected_prob = probs[word_id]\n",
        "        log_probabilities.append(math.log(selected_prob))\n",
        "\n",
        "    perplexity = math.exp(-np.mean(log_probabilities))\n",
        "    your_perplexity = model.perplexity(tok_validation_dataset[:10])\n",
        "    assert abs(perplexity-your_perplexity) < 0.1, \"your perplexity does not \" + \\\n",
        "    \"match the one we calculated from `next_word_probabilities`,\\n\" + \\\n",
        "    \"at least one of `perplexity` or `next_word_probabilities` is incorrect.\\n\" + \\\n",
        "    f\"we calcuated {perplexity} from `next_word_probabilities`,\\n\" + \\\n",
        "    f\"but your perplexity function returned {your_perplexity} (on a small sample).\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtivHzytNbwN"
      },
      "outputs": [],
      "source": [
        "check_validity(unigram_demonstration_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQDw7X0RNc-x"
      },
      "source": [
        "To generate from a language model, we can sample one word at a time conditioning on the words we have generated so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxTfelePNd4Q"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, n=20, prefix=('<eos>', '<eos>')):\n",
        "    prefix = list(prefix)\n",
        "    for _ in range(n):\n",
        "        probs = model.next_word_probabilities(prefix)\n",
        "        word = random.choices(vocab.all_tokens, probs)[0]\n",
        "        prefix.append(word)\n",
        "    return ' '.join(prefix)\n",
        "\n",
        "# unigram model does not utilize prefix\n",
        "print(generate_text(unigram_demonstration_model, prefix=\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkz8XPPkNjaF"
      },
      "source": [
        "TODO: Copy the printed output to your report.\n",
        "\n",
        "In fact there are many strategies to get better-sounding samples, such as only sampling from the top-k words or sharpening the distribution with a temperature.  You can read more about sampling from a language model in this recent paper: https://arxiv.org/pdf/1904.09751.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCGwzn0PNqhw"
      },
      "source": [
        "You will need to submit some outputs from the models you implement for us to grade.  The following function will be used to generate the required output files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5QOEDdhNjwp"
      },
      "outputs": [],
      "source": [
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_prefixes_short.txt\n",
        "!wget https://cal-cs288.github.io/sp21/project_files/proj_1/eval_output_vocab_short.txt\n",
        "\n",
        "def save_truncated_distribution(model, filename, short=True):\n",
        "    \"\"\"Generate a file of truncated distributions.\n",
        "\n",
        "    Probability distributions over the full vocabulary are large,\n",
        "    so we will truncate the distribution to a smaller vocabulary.\n",
        "\n",
        "    Please do not edit this function\n",
        "    \"\"\"\n",
        "    vocab_name = 'eval_output_vocab'\n",
        "    prefixes_name = 'eval_prefixes'\n",
        "\n",
        "    if short:\n",
        "      vocab_name += '_short'\n",
        "      prefixes_name += '_short'\n",
        "\n",
        "    with open(f'{vocab_name}.txt', 'r') as eval_vocab_file:\n",
        "        eval_vocab = [w.strip() for w in eval_vocab_file]\n",
        "    eval_vocab_ids = sorted(list(set([vocab.str_to_id[s] if s in vocab else vocab.str_to_id[vocab.unk_tok]\n",
        "                      for s in eval_vocab])))\n",
        "\n",
        "    all_selected_probabilities = []\n",
        "    with open(f'{prefixes_name}.txt', 'r') as eval_prefixes_file:\n",
        "        lines = eval_prefixes_file.readlines()\n",
        "        for line in tqdm.notebook.tqdm(lines, leave=False):\n",
        "            prefix = line.strip().split(' ')\n",
        "            probs = model.next_word_probabilities(prefix)\n",
        "            selected_probs = np.array([probs[i] for i in eval_vocab_ids], dtype=np.float32)\n",
        "            all_selected_probabilities.append(selected_probs)\n",
        "\n",
        "    all_selected_probabilities = np.stack(all_selected_probabilities)\n",
        "    np.save(filename, all_selected_probabilities)\n",
        "    print('saved', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrNe6jdANxly"
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(unigram_demonstration_model,\n",
        "                            'unigram_demonstration_predictions.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hIKjvmjNzbU"
      },
      "source": [
        "### N-gram Model\n",
        "\n",
        "Now it's time to implement an n-gram language model.\n",
        "\n",
        "Because not every n-gram will have been observed in training, use add-alpha smoothing to make sure no output word has probability 0.\n",
        "\n",
        "This is an example of bigram model with smoothing:\n",
        "$$P(w_2|w_1)=\\frac{C(w_1,w_2)+\\alpha}{C(w_1)+N\\alpha}$$\n",
        "\n",
        "where $N$ is the vocab size and $C$ is the count for the given unigram/bigram.  An alpha value around `3e-3`  should work.  Later, we'll replace this smoothing with model backoff.\n",
        "\n",
        "One **edge case** you will need to handle is at the beginning of the text where you don't have `n-1` prior words.  You may handle this by using a uniform distribution over the vocabulary.\n",
        "\n",
        "A properly implemented bi-gram model should get a perplexity about/below **635** on the validation set.\n",
        "\n",
        "**Note**: Do not change the signature of the `next_word_probabilities` and `perplexity` functions.  We will use these as a common interface for all of the different model types.  Make sure these two functions call `n_gram_probability`, because later we are going to override `n_gram_probability` in a subclass.\n",
        "Also, we suggest pre-computing and caching the counts $C$ when you initialize `NGramModel` for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ658ednN1rH"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "class NGramModel:\n",
        "    def __init__(self, train_text: List[str], n: int = 2, alpha: float = 3e-3):\n",
        "        \"\"\"\n",
        "        Initializes the model, computes n-gram and (n-1)-gram counts from the training text.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.smoothing = alpha\n",
        "        self.vocab_size = len(set(train_text))  # Size of the unique vocabulary\n",
        "        self.train_text = train_text\n",
        "\n",
        "        # Store n-gram and (n-1)-gram counts\n",
        "        self.ngram_freq = defaultdict(int)\n",
        "        self.n_minus_1_freq = defaultdict(int)\n",
        "\n",
        "        # Count n-grams and (n-1)-grams from the training text\n",
        "        for i in range(len(train_text) - n + 1):\n",
        "            ngram = tuple(train_text[i:i + n])\n",
        "            self.ngram_freq[ngram] += 1\n",
        "\n",
        "            if n > 1:\n",
        "                n_minus_1_gram = tuple(train_text[i:i + n - 1])\n",
        "                self.n_minus_1_freq[n_minus_1_gram] += 1\n",
        "\n",
        "    def n_gram_probability(self, n_gram: Tuple[str, ...]):\n",
        "        \"\"\"\n",
        "        Returns the smoothed probability of the n-gram.\n",
        "        \"\"\"\n",
        "        if self.n == 1:\n",
        "            unigram = (n_gram[-1],)\n",
        "            unigram_count = self.ngram_freq[unigram]\n",
        "            total_unigrams = sum(self.ngram_freq.values())\n",
        "            prob = (unigram_count + self.smoothing) / (total_unigrams + self.smoothing * self.vocab_size)\n",
        "            return prob\n",
        "\n",
        "        # For n-grams (n > 1)\n",
        "        assert len(n_gram) == self.n, f\"Expected {self.n}-gram, got {n_gram}\"\n",
        "\n",
        "        n_minus_1_gram = n_gram[:-1]\n",
        "        ngram_count = self.ngram_freq[n_gram]\n",
        "        n_minus_1_gram_count = self.n_minus_1_freq[n_minus_1_gram]\n",
        "\n",
        "        # Apply smoothing\n",
        "        prob = (ngram_count + self.smoothing) / (n_minus_1_gram_count + self.smoothing * self.vocab_size)\n",
        "        return prob\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix: List[str]):\n",
        "        \"\"\"\n",
        "        Returns a list of probabilities for each word in the vocabulary given the text prefix.\n",
        "        \"\"\"\n",
        "        if len(text_prefix) < self.n - 1:\n",
        "            # If the prefix is shorter than expected, return uniform probabilities\n",
        "            return [1 / self.vocab_size] * self.vocab_size\n",
        "\n",
        "        # Use the last (n-1) tokens from the prefix for context\n",
        "        n_minus_1_gram = tuple(text_prefix[-(self.n - 1):])\n",
        "\n",
        "        # Compute probabilities for each word in the vocabulary\n",
        "        probabilities = []\n",
        "        for word in vocab.all_tokens:\n",
        "            ngram = n_minus_1_gram + (word,)\n",
        "            probabilities.append(self.n_gram_probability(ngram))\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def perplexity(self, full_text: List[str]):\n",
        "        \"\"\"\n",
        "        Returns the perplexity for the given text.\n",
        "        \"\"\"\n",
        "        log_probs = []\n",
        "\n",
        "        # Loop through the text to calculate log probabilities\n",
        "        for i in range(len(full_text) - self.n + 1):\n",
        "            ngram = tuple(full_text[i:i + self.n])\n",
        "            prob = self.n_gram_probability(ngram)\n",
        "            log_probs.append(math.log(prob, 2))\n",
        "\n",
        "        # Handle the first n-1 tokens by assuming a uniform distribution\n",
        "        uniform_log_prob = math.log(1 / self.vocab_size, 2)\n",
        "        log_probs = [uniform_log_prob] * (self.n - 1) + log_probs\n",
        "\n",
        "        # Compute perplexity\n",
        "        return 2 ** (-np.mean(log_probs))\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "unigram_model = NGramModel(tok_train_dataset, n=1)  # Equivalent to a Unigram model\n",
        "check_validity(unigram_model)\n",
        "print('unigram validation perplexity:', unigram_model.perplexity(tok_validation_dataset))\n",
        "\n",
        "bigram_model = NGramModel(tok_train_dataset, n=2)  # Bigram model\n",
        "check_validity(bigram_model)\n",
        "print('bigram validation perplexity:', bigram_model.perplexity(tok_validation_dataset))\n",
        "\n",
        "trigram_model = NGramModel(tok_train_dataset, n=3)  # Trigram model\n",
        "check_validity(trigram_model)\n",
        "print('trigram validation perplexity:', trigram_model.perplexity(tok_validation_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rkm0hq9OzQq"
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(bigram_model, 'bigram_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yly_IiuYOb5w"
      },
      "source": [
        "Please download `bigram_predictions.npy` once you finish this section so that you can submit it.\n",
        "\n",
        "In the block below, please report your bigram validation perplexity.  (We will use this to help us calibrate our scoring on the test set.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSMqV2iJOetO"
      },
      "source": [
        "TODO: Report the perplexity in your report.\n",
        "\n",
        "Bigram validation perplexity: **635.624422688812**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYCYFh4VOjAa"
      },
      "source": [
        "We can also generate samples from the model to get an idea of how it is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2kLtZ2wOc1X"
      },
      "outputs": [],
      "source": [
        "print(generate_text(bigram_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhLE7bjdOnPA"
      },
      "source": [
        "We now free up some RAM, **it is important to run the cell below, otherwise you will likely run out of RAM in the Colab runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "464Sc0I0Oou1"
      },
      "outputs": [],
      "source": [
        "# Free up some RAM.\n",
        "del bigram_model\n",
        "del trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eszOSFHbOqRq"
      },
      "source": [
        "This basic model works okay for bigrams, but a better strategy (especially for higher-order models) is to use backoff.  Implement backoff with absolute discounting.\n",
        "$$P\\left(w_i|w_{i-n+1}^{i-1}\\right)=\\frac{max\\left\\{C(w_{i-n+1}^i)-\\delta,0\\right\\}}{\\sum_{w_i} C(w_{i-n+1}^i)} + \\alpha(w_{i-n+1}^{i-1}) P(w_i|w_{i-n+2}^{i-1})$$\n",
        "\n",
        "$$\\alpha\\left(w_{i-n+1}^{i-1}\\right)=\\frac{\\delta N_{1+}(w_{i-n+1}^{i-1})}{{\\sum_{w_i} C(w_{i-n+1}^i)}}$$\n",
        "where $N_{1+}$ is the number of words that appear after the previous $n-1$ words (the number of times the max will select something other than 0 in the first equation).  If $\\sum_{w_i} C(w_{i-n+1}^i)=0$, use the lower order model probability directly (the above equations would have a division by 0).\n",
        "\n",
        "We found a discount $\\delta$ of 0.9 to work well based on validation performance.  A trigram model with this discount value should get a validation perplexity around/below **310**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBCxZRcnOqy5"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "class DiscountBackoffModel(NGramModel):\n",
        "    def __init__(self, train_text: List[str],\n",
        "                 lower_order_model: Union[NGramModel, \"DiscountBackoffModel\"],\n",
        "                 n: int = 2,\n",
        "                 delta: float = 0.9):\n",
        "        \"\"\"We only use n >= 2 for backoff\"\"\"\n",
        "        super().__init__(train_text, n=n)\n",
        "        assert n >= 2, \"N must be 2 or greater for backoff\"\n",
        "\n",
        "        self.lower_order_model = lower_order_model\n",
        "        self.discount = delta\n",
        "\n",
        "        # Initialize structures to store counts\n",
        "        self.context_counts = defaultdict(int)\n",
        "        self.n_gram_counts = defaultdict(lambda: defaultdict(int))\n",
        "        self.N1_plus = defaultdict(int)\n",
        "\n",
        "        # Precompute counts for n-grams and (n-1)-gram contexts\n",
        "        self.initialize_counts(train_text)\n",
        "\n",
        "    def initialize_counts(self, train_text: List[str]):\n",
        "        \"\"\"Compute counts of n-grams, contexts, and distinct word counts.\"\"\"\n",
        "        for idx in range(len(train_text) - self.n + 1):\n",
        "            # Extract (n-1)-gram context and the nth word\n",
        "            context = tuple(train_text[idx:idx+self.n-1])\n",
        "            word = train_text[idx + self.n - 1]\n",
        "\n",
        "            # Update context and n-gram counts\n",
        "            self.context_counts[context] += 1\n",
        "            self.n_gram_counts[context][word] += 1\n",
        "\n",
        "            # Track the number of distinct words following each context\n",
        "            if self.n_gram_counts[context][word] == 1:\n",
        "                self.N1_plus[context] += 1\n",
        "\n",
        "    def n_gram_probability(self, n_gram: Tuple[str, ...]) -> float:\n",
        "        \"\"\"Calculate the probability of the last word in an n-gram using backoff.\"\"\"\n",
        "        context = n_gram[:-1]\n",
        "        word = n_gram[-1]\n",
        "\n",
        "        # If context has no counts, back off to the lower-order model\n",
        "        if context not in self.context_counts or self.context_counts[context] == 0:\n",
        "            return self.lower_order_model.n_gram_probability(n_gram[1:])\n",
        "\n",
        "        # Get counts for the context and the current n-gram\n",
        "        context_count = self.context_counts[context]\n",
        "        word_count = self.n_gram_counts[context].get(word, 0)\n",
        "\n",
        "        # Apply discount to the word count\n",
        "        discounted_count = max(word_count - self.discount, 0)\n",
        "        discounted_prob = discounted_count / context_count\n",
        "\n",
        "        # Calculate backoff weight (alpha)\n",
        "        alpha = (self.discount * self.N1_plus[context]) / context_count\n",
        "\n",
        "        # Get the backoff probability from the lower-order model\n",
        "        backoff_prob = self.lower_order_model.n_gram_probability(n_gram[1:])\n",
        "\n",
        "        # Combine discounted and backoff probabilities\n",
        "        final_prob = discounted_prob + alpha * backoff_prob\n",
        "\n",
        "        # Ensure probability is bounded between 0 and 1\n",
        "        return max(0, min(1, final_prob))\n",
        "\n",
        "bigram_backoff_model = DiscountBackoffModel(tok_train_dataset, unigram_model, 2)\n",
        "check_validity(bigram_backoff_model)\n",
        "print('bigram backoff validation perplexity:', bigram_backoff_model.perplexity(tok_validation_dataset))\n",
        "\n",
        "trigram_backoff_model = DiscountBackoffModel(tok_train_dataset, bigram_backoff_model, 3)\n",
        "check_validity(trigram_backoff_model)\n",
        "print('trigram backoff validation perplexity:', trigram_backoff_model.perplexity(tok_validation_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDhC-K0DPQb2"
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(trigram_backoff_model, 'trigram_backoff_predictions.npy') # this might take a few minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8hUz6UPReE"
      },
      "source": [
        "TODO: Report your trigram backoff model perplexity.\n",
        "\n",
        "Trigram backoff validation perplexity: **310.7262767493293**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvB7YH5rPT10"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xihQb9fNPSSl"
      },
      "outputs": [],
      "source": [
        "# Release models we don't need any more.\n",
        "del unigram_model\n",
        "del bigram_backoff_model\n",
        "del trigram_backoff_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAc_3o4CQVXm"
      },
      "source": [
        "### Neural N-gram Model\n",
        "\n",
        "In this section, you will implement a neural version of an n-gram model.  The model will use a simple feedforward neural network that takes the previous `n-1` words and outputs a distribution over the next word.\n",
        "\n",
        "You will use PyTorch to implement the model.  We've provided a little bit of code to help with the data loading using PyTorch's data loaders (https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "A model with the following architecture and hyperparameters should reach a validation perplexity around/below **240**.\n",
        "* embed the words with dimension 128, then flatten into a single embedding for $n-1$ words (with size $(n-1)*128$)\n",
        "* run 2 hidden layers with 1024 hidden units, then project down to size 128 before the final layer (ie. 4 layers total).\n",
        "* use weight tying for the embedding and final linear layer (this made a very large difference in our experiments); you can do this by creating the output layer with `nn.Linear`, then using `F.embedding` with the linear layer's `.weight` to embed the input\n",
        "* rectified linear activation (ReLU) and dropout 0.1 after first 2 hidden layers. **Note: You will likely find a performance drop if you add a nonlinear activation function after the dimension reduction layer.**\n",
        "* train for 10 epochs with the Adam optimizer (should take around 15-20 minutes)\n",
        "* do early stopping based on validation set perplexity.\n",
        "\n",
        "\n",
        "We encourage you to try other architectures and hyperparameters, and you will likely find some that work better than the ones listed above.  A proper implementation with these should be enough to receive full credit on the assignment, though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBbGkdl-QY8_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from typing import List\n",
        "import tqdm\n",
        "\n",
        "\n",
        "# Dataset class to handle N-Gram input/output pairs\n",
        "class NeuralNgramDataset(Dataset):\n",
        "    def __init__(self, text_token_ids: List[int], n: int):\n",
        "        self.token_ids = text_token_ids\n",
        "        self.n_gram = n\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        # Handling cases where index is less than n-1\n",
        "        if index < self.n_gram - 1:\n",
        "            padding = [vocab.str_to_id[vocab.eos_tok]] * (self.n_gram - index - 1)\n",
        "            previous_tokens = padding + self.token_ids[:index]\n",
        "        else:\n",
        "            previous_tokens = self.token_ids[index - self.n_gram + 1:index]\n",
        "\n",
        "        assert len(previous_tokens) == self.n_gram - 1, previous_tokens\n",
        "\n",
        "        x_tensor = torch.tensor(previous_tokens, dtype=torch.long)\n",
        "        y_tensor = torch.tensor(self.token_ids[index], dtype=torch.long)\n",
        "        return x_tensor, y_tensor\n",
        "\n",
        "\n",
        "# Recurrent Neural Network-based N-Gram Model\n",
        "class NeuralNGramNetwork(nn.Module):\n",
        "    def __init__(self, n: int, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 1024, dropout_rate: float = 0.1):\n",
        "        super(NeuralNGramNetwork, self).__init__()\n",
        "        self.n_gram = n\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Using GRU for sequential modeling\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=dropout_rate)\n",
        "\n",
        "        # Layer to project GRU hidden states to embedding dimension\n",
        "        self.hidden_to_embedding = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "        # Output layer for classification\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Weight tying to share the embedding weights with output layer\n",
        "        self.output_layer.weight = self.embedding_layer.weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_seq = self.embedding_layer(x)  # Shape: (batch_size, n-1, embed_dim)\n",
        "\n",
        "        # Forward pass through the GRU layer\n",
        "        rnn_output, _ = self.gru(embedded_seq)\n",
        "\n",
        "        # Take the last hidden state of GRU\n",
        "        final_hidden_state = rnn_output[:, -1, :]  # Shape: (batch_size, hidden_dim)\n",
        "\n",
        "        # Project to embedding dimension\n",
        "        projected_output = self.hidden_to_embedding(final_hidden_state)  # Shape: (batch_size, embed_dim)\n",
        "\n",
        "        # Calculate logits\n",
        "        logits = self.output_layer(projected_output)  # Shape: (batch_size, vocab_size)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Wrapper class for training and evaluation\n",
        "class NeuralNGramModel:\n",
        "    def __init__(self, n: int, device: str = \"cpu\", **model_configs):\n",
        "        self.n_gram = n\n",
        "        self.device = device\n",
        "        vocab_size = len(vocab.str_to_id)\n",
        "\n",
        "        self.network = NeuralNGramNetwork(n, vocab_size, **model_configs).to(self.device)\n",
        "\n",
        "    # Modified training loop with some structure and handling\n",
        "    def train(self, n_epoch: int = 10, lr: float = 0.001, batch_size: int = 128, grad_clip: float = 0.5, weight_decay: float = 1e-5):\n",
        "        # Dataset and DataLoader creation\n",
        "        train_dataset = NeuralNgramDataset(vocab.strs_to_ids(tok_train_dataset), self.n_gram)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Optimizer and scheduler setup\n",
        "        optimizer = torch.optim.Adam(self.network.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(n_epoch):\n",
        "            epoch_loss = 0\n",
        "            self.network.train()\n",
        "\n",
        "            # Loop over batches\n",
        "            for x_batch, y_batch in tqdm.notebook.tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
        "\n",
        "                # Zero gradients, forward pass, compute loss, and backpropagate\n",
        "                optimizer.zero_grad()\n",
        "                logits = self.network(x_batch)\n",
        "                loss = loss_function(logits, y_batch)\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), grad_clip)\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            # Scheduler step based on epoch loss\n",
        "            scheduler.step(epoch_loss / len(train_loader))\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{n_epoch}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Generate next word probabilities based on a given text prefix\n",
        "    def next_word_probabilities(self, text_prefix: List[str]) -> List[float]:\n",
        "        self.network.eval()\n",
        "\n",
        "        # Convert input text prefix to token IDs\n",
        "        token_ids = vocab.strs_to_ids(text_prefix)\n",
        "\n",
        "        # Pad the sequence if it's shorter than expected\n",
        "        if len(token_ids) < self.n_gram - 1:\n",
        "            pad = [vocab.str_to_id[vocab.eos_tok]] * (self.n_gram - 1 - len(token_ids))\n",
        "            token_ids = pad + token_ids\n",
        "\n",
        "        # Convert token IDs to tensor and pass through the model\n",
        "        input_tensor = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.network(input_tensor)\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probabilities = F.softmax(logits, dim=-1).squeeze(0).tolist()\n",
        "        return probabilities\n",
        "\n",
        "    # Perplexity evaluation method\n",
        "    def perplexity(self, text: List[str]) -> float:\n",
        "        self.network.eval()\n",
        "\n",
        "        # Create dataset and data loader for the text\n",
        "        test_dataset = NeuralNgramDataset(vocab.strs_to_ids(text), self.n_gram)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "\n",
        "        total_loss = 0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Evaluate model on validation data\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in test_loader:\n",
        "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
        "\n",
        "                logits = self.network(x_batch)\n",
        "                loss = criterion(logits, y_batch)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss and convert to perplexity\n",
        "        avg_loss = total_loss / len(test_loader)\n",
        "        perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "        return perplexity.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model for a trigram setup\n",
        "neural_trigram_model = NeuralNGramModel(3,device=\"cuda\")\n",
        "neural_trigram_model.train(lr=5e-4)\n",
        "# Calculate and print perplexity on the validation set\n",
        "print('Neural trigram validation perplexity:', neural_trigram_model.perplexity(tok_validation_dataset))"
      ],
      "metadata": {
        "id": "I7VRI438B_YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VpQ4hvFRW4l"
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(neural_trigram_model, 'neural_trigram_predictions.npy', short=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh3AcU7NRa3R"
      },
      "source": [
        "TODO: Fill in your neural trigram perplexity in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mTSO0jyRdcS"
      },
      "source": [
        "<!-- Do not remove this comment, it is used by the autograder: RqYJKsoTS6 -->\n",
        "\n",
        "Neural trigram validation perplexity: 240.79531860351562"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A78a5cEtRetf"
      },
      "source": [
        "Free up RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEZzP2GaRtCG"
      },
      "outputs": [],
      "source": [
        "# Delete model we don't need.\n",
        "del neural_trigram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-pjInimRu4B"
      },
      "source": [
        "### LSTM Model\n",
        "\n",
        "For this stage of the project, you will implement an LSTM language model.\n",
        "\n",
        "For recurrent language modeling, the data batching strategy is a bit different from what is used in some other tasks.  Sentences are concatenated together so that one sentence starts right after the other, and an unfinished sentence will be continued in the next batch.\n",
        "To properly deal with this input format, you should **save the last state of the LSTM from a batch to feed in as the first state of the next batch**.  When you save state across different batches, you should call `.detach()` on the state tensors before the next batch to tell PyTorch not to backpropagate gradients through the state into the batch you have already finished (which will cause a runtime error).\n",
        "\n",
        "We expect your model to reach a validation perplexity around/below **214**.\n",
        "The following architecture and hyperparameters should be sufficient to get there.\n",
        "* 3 LSTM layers with 512 units\n",
        "* dropout of 0.5 after each LSTM layer\n",
        "* instead of projecting directly from the last LSTM output to the vocabulary size for softmax, project down to a smaller size first (e.g. 512->128->vocab_size). **NOTE: You may find that adding nonlinearities between these layers can hurt performance, try without first.**\n",
        "* use the same weights for the embedding layer and the pre-softmax layer; dimension 128\n",
        "* train with Adam (using default learning rates) for at least 20 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0osx3E71Rvku"
      },
      "outputs": [],
      "source": [
        "# ref: https://github.com/pytorch/text/blob/0.5.0/torchtext/data/iterator.py#L173\n",
        "\n",
        "class LstmDataIterator:\n",
        "    def __init__(self, dataset: List[int], batch_size: int = 64, seq_len: int = 32, device: str = \"cpu\"):\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.device = device\n",
        "\n",
        "        # pad the dataset so that it is divisible by batch_size\n",
        "        dataset = dataset + [vocab.str_to_id[vocab.pad_tok]] * (math.ceil(len(dataset) / batch_size) * batch_size - len(dataset))\n",
        "\n",
        "        self.n_samples = math.ceil(\n",
        "            (len(dataset) // batch_size - 1) / seq_len\n",
        "        )\n",
        "\n",
        "        dataset = torch.tensor(dataset, dtype=torch.long)\n",
        "        self.dataset = dataset.view(batch_size, -1).t().contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        start = i * self.seq_len\n",
        "        end = min(start + self.seq_len, self.dataset.shape[0] - 1)\n",
        "\n",
        "        inputs = self.dataset[start : end]\n",
        "        outputs = self.dataset[start + 1 : end + 1]\n",
        "        assert inputs.shape == outputs.shape, f\"{i}: {inputs.shape} {outputs.shape}\"\n",
        "        # (seq_len, batch_size)\n",
        "        return inputs.to(self.device), outputs.to(self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUmIk9cZR84N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "class LSTMNetwork(nn.Module):\n",
        "    # A PyTorch Module that holds the neural network for your model\n",
        "\n",
        "    def __init__(self, embed_dim: int = 128, n_layer: int = 3, hidden_dim: int = 512, dropout_rate: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        vocab_size = len(vocab.str_to_id)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layer,\n",
        "            dropout=dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Layer normalization for stabilization\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Projection layers\n",
        "        self.proj = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "        # Output layer (tied weights with embedding layer)\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
        "        self.output_layer.weight = self.embedding.weight  # Weight tying\n",
        "\n",
        "        # Weight initialization\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
        "        \"\"\"Compute the output of the network.\"\"\"\n",
        "        embedded = self.embedding(x)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        # Pass through LSTM layers\n",
        "        lstm_out, state = self.lstm(embedded, state)  # lstm_out: (seq_len, batch_size, hidden_dim)\n",
        "\n",
        "        # Apply layer normalization\n",
        "        lstm_out = self.layer_norm(lstm_out)\n",
        "\n",
        "        # Project to embedding dimension\n",
        "        proj_out = self.proj(lstm_out)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = self.output_layer(proj_out)  # (seq_len, batch_size, vocab_size)\n",
        "\n",
        "        # Compute log probabilities\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        return log_probs, state\n",
        "\n",
        "\n",
        "class LSTMModel:\n",
        "    \"A class that wraps LSTMNetwork to handle training and evaluation.\"\n",
        "\n",
        "    def __init__(self, device: str = \"cpu\", **model_configs):\n",
        "        self.device = device\n",
        "        if \"cuda\" in self.device:\n",
        "            assert torch.cuda.is_available(), \"no GPU found, in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator\"\n",
        "\n",
        "        self.network = LSTMNetwork(**model_configs).to(self.device)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        n_epoch: int = 20, lr: float = 1e-3, batch_size: int = 64, seq_len: int = 32\n",
        "    ):\n",
        "        train_data_iter = LstmDataIterator(vocab.strs_to_ids(tok_train_dataset), batch_size, seq_len, self.device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(self.network.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        criterion = nn.NLLLoss(ignore_index=vocab.str_to_id[vocab.pad_tok])\n",
        "\n",
        "        self.network.train()\n",
        "\n",
        "        for epoch in range(n_epoch):\n",
        "            hidden_state = None  # Initialize hidden state at the beginning of each epoch\n",
        "            total_loss = 0.0\n",
        "\n",
        "            for i in range(len(train_data_iter)):\n",
        "                inputs, targets = train_data_iter[i]  # (seq_len, batch_size)\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                log_probs, hidden_state = self.network(inputs, hidden_state)\n",
        "\n",
        "                # Reshape for loss computation\n",
        "                loss = criterion(\n",
        "                    log_probs.view(-1, log_probs.size(2)),  # (seq_len * batch_size, vocab_size)\n",
        "                    targets.view(-1),  # (seq_len * batch_size)\n",
        "                )\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to avoid exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Detach hidden state to prevent backprop through entire sequence\n",
        "                hidden_state = tuple(h.detach() for h in hidden_state)\n",
        "\n",
        "            avg_loss = total_loss / len(train_data_iter)\n",
        "            print(f\"Epoch {epoch + 1}/{n_epoch}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix: List[str]):\n",
        "        \"Return a list of probabilities for each word in the vocabulary.\"\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            ids_prefix = torch.tensor(\n",
        "                vocab.strs_to_ids(text_prefix), dtype=torch.long, device=self.device\n",
        "            ).view(-1, 1)  # (seq_len, batch_size=1)\n",
        "\n",
        "            # Initialize hidden state\n",
        "            hidden_state = None\n",
        "\n",
        "            # Forward pass\n",
        "            log_probs, hidden_state = self.network(ids_prefix, hidden_state)\n",
        "\n",
        "            # Get the last timestep's log probabilities\n",
        "            last_log_probs = log_probs[-1, 0, :]  # (vocab_size,)\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probabilities = last_log_probs.exp().cpu().numpy()\n",
        "            return probabilities.tolist()\n",
        "\n",
        "    def dataset_perplexity(self, dataset: List[str], batch_size: int = 64, seq_len: int = 32):\n",
        "        \"Return perplexity as a float.\"\n",
        "        self.network.eval()\n",
        "        data_iterator = LstmDataIterator(\n",
        "            vocab.strs_to_ids(dataset), batch_size, seq_len, self.device\n",
        "        )\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "        criterion = nn.NLLLoss(\n",
        "            ignore_index=vocab.str_to_id[vocab.pad_tok], reduction=\"sum\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hidden_state = None\n",
        "            for i in range(len(data_iterator)):\n",
        "                inputs, targets = data_iterator[i]\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                log_probs, hidden_state = self.network(inputs, hidden_state)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(\n",
        "                    log_probs.view(-1, log_probs.size(2)),\n",
        "                    targets.view(-1),\n",
        "                )\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Count non-padding tokens\n",
        "                non_pad_mask = targets.view(-1) != vocab.str_to_id[vocab.pad_tok]\n",
        "                total_tokens += non_pad_mask.sum().item()\n",
        "\n",
        "                # Detach hidden state\n",
        "                hidden_state = tuple(h.detach() for h in hidden_state)\n",
        "\n",
        "        # Calculate perplexity\n",
        "        avg_loss = total_loss / total_tokens\n",
        "        perplexity = math.exp(avg_loss)\n",
        "        return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO1VxBPVT7VJ"
      },
      "outputs": [],
      "source": [
        "lstm_model = LSTMModel(device=\"cuda\")\n",
        "lstm_model.train()\n",
        "\n",
        "print('lstm validation perplexity:', lstm_model.dataset_perplexity(tok_validation_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-ek0DGjUYx2"
      },
      "outputs": [],
      "source": [
        "save_truncated_distribution(lstm_model, 'lstm_predictions.npy', short=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SZRaWalUb15"
      },
      "source": [
        "TODO: Report your LSTM perplexity.\n",
        "\n",
        "LSTM validation perplexity: 166.70655249864177"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr1HkFsSUkDI"
      },
      "source": [
        "# Experimentation: 1-Page Report\n",
        "\n",
        "Now it's time for you to experiment.  Try to reach a validation perplexity below 120. You may either modify the LSTM class above, or copy it down to the code cell below and modify it there. Just **be sure to run code cell below to generate results with your improved LSTM**.  \n",
        "\n",
        "It is okay if the bulk of your improvements are due to hyperparameter tuning (such as changing number or sizes of layers), but implement at least one more substantial change to the model.  Here are some ideas (several of which come from https://arxiv.org/pdf/1708.02182.pdf):\n",
        "* activation regularization - add a l2 regularization penalty on the activation of the LSTM output (standard l2 regularization is on the weights)\n",
        "* weight-drop regularization - apply dropout to the weight matrices instead of activations\n",
        "* learning rate scheduling - decrease the learning rate during training\n",
        "* embedding dropout - zero out the entire embedding for a random set of words in the embedding matrix\n",
        "* ensembling - average the predictions of several models trained with different initialization random seeds\n",
        "* temporal activation regularization - add l2 regularization on the difference between the LSTM output activations at adjacent timesteps\n",
        "\n",
        "You may notice that most of these suggestions are regularization techniques.  This dataset is considered fairly small, so regularization is one of the best ways to improve performance.\n",
        "\n",
        "TODO: In the report, submit a write-up describing the extensions and/or modifications that you tried.  Your description should be **1-page maximum** in length.\n",
        "For full credit, your write-up should include:\n",
        "1.   A concise and precise description of the extension that you tried.\n",
        "2.   A motivation for why you believed this approach might improve your model.\n",
        "3.   A discussion of whether the extension was effective and/or an analysis of the results.  This will generally involve some combination of tables, learning curves, etc.\n",
        "4.   A bottom-line summary of your results comparing validation perplexities of your improvement to the original LSTM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16nc1WTIUqKL"
      },
      "source": [
        "Run the cell below in order to train your improved LSTM and evaluate it.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ref: https://github.com/pytorch/text/blob/0.5.0/torchtext/data/iterator.py#L173\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import math\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "class LstmDataIterator:\n",
        "    def __init__(self, dataset: List[int], batch_size: int = 64, seq_len: int = 32, device: str = \"cpu\"):\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.device = device\n",
        "\n",
        "        # pad the dataset so that it is divisible by batch_size\n",
        "        dataset = dataset + [vocab.str_to_id[vocab.pad_tok]] * (math.ceil(len(dataset) / batch_size) * batch_size - len(dataset))\n",
        "\n",
        "        self.n_samples = math.ceil(\n",
        "            (len(dataset) // batch_size - 1) / seq_len\n",
        "        )\n",
        "\n",
        "        dataset = torch.tensor(dataset, dtype=torch.long)\n",
        "        self.dataset = dataset.view(batch_size, -1).t().contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        start = i * self.seq_len\n",
        "        end = min(start + self.seq_len, self.dataset.shape[0] - 1)\n",
        "\n",
        "        inputs = self.dataset[start : end]\n",
        "        outputs = self.dataset[start + 1 : end + 1]\n",
        "        assert inputs.shape == outputs.shape, f\"{i}: {inputs.shape} {outputs.shape}\"\n",
        "        # (seq_len, batch_size)\n",
        "        return inputs.to(self.device), outputs.to(self.device)\n",
        "\n",
        "\n",
        "class LSTMNetwork(nn.Module):\n",
        "    def __init__(self, embed_dim: int = 128, n_layer: int = 3, hidden_dim: int = 512, dropout_rate: float = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        vocab_size = len(vocab.str_to_id)\n",
        "\n",
        "        # Embedding layer with dropout for regularization\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "        self.embed_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # LSTM layer with weight dropout regularization\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=n_layer,\n",
        "            dropout=0,  # Dropout will be applied to weights\n",
        "        )\n",
        "        self.weight_dropout = nn.Dropout(dropout_rate)  # Dropout applied to weights in the LSTM\n",
        "\n",
        "        # Projection layer\n",
        "        self.proj = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "        # Output layer (tied weights with embedding layer)\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
        "        self.output_layer.weight = self.embedding.weight  # Weight tying\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
        "        # Apply embedding dropout\n",
        "        embedded = self.embed_dropout(self.embedding(x))  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        lstm_out, state = self.lstm(embedded, state)  # (seq_len, batch_size, hidden_dim)\n",
        "\n",
        "        # Apply weight dropout regularization\n",
        "        lstm_out = self.weight_dropout(lstm_out)\n",
        "\n",
        "        # Project to embedding dimension\n",
        "        proj_out = self.proj(lstm_out)  # (seq_len, batch_size, embed_dim)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = self.output_layer(proj_out)  # (seq_len, batch_size, vocab_size)\n",
        "\n",
        "        # Compute log probabilities\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        return log_probs, state\n",
        "\n",
        "\n",
        "class LSTMModel:\n",
        "    def __init__(self, device: str = \"cpu\", **model_configs):\n",
        "        self.device = device\n",
        "        if \"cuda\" in self.device:\n",
        "            assert torch.cuda.is_available(), \"No GPU found, in Colab go to 'Edit->Notebook settings' and choose a GPU hardware accelerator\"\n",
        "\n",
        "        self.network = LSTMNetwork(**model_configs).to(self.device)\n",
        "        self.scheduler = ReduceLROnPlateau(optimizer=optim.Adam(self.network.parameters(), lr=1e-3),\n",
        "                                           mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "    def train(self, n_epoch: int = 20, lr: float = 1e-3, batch_size: int = 64, seq_len: int = 32, grad_clip: float = 5.0):\n",
        "        train_data_iter = LstmDataIterator(vocab.strs_to_ids(tok_train_dataset), batch_size, seq_len, self.device)\n",
        "        optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
        "        criterion = nn.NLLLoss(ignore_index=vocab.str_to_id[vocab.pad_tok])\n",
        "\n",
        "        for epoch in range(n_epoch):\n",
        "            hidden_state = None  # Initialize hidden state at the beginning of each epoch\n",
        "            total_loss = 0.0\n",
        "\n",
        "            self.network.train()\n",
        "\n",
        "            for i in range(len(train_data_iter)):\n",
        "                inputs, targets = train_data_iter[i]\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                log_probs, hidden_state = self.network(inputs, hidden_state)\n",
        "\n",
        "                # Reshape for loss computation\n",
        "                loss = criterion(\n",
        "                    log_probs.view(-1, log_probs.size(2)),  # (seq_len * batch_size, vocab_size)\n",
        "                    targets.view(-1),  # (seq_len * batch_size)\n",
        "                )\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), grad_clip)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Detach hidden state to prevent backprop through entire sequence\n",
        "                hidden_state = tuple(h.detach() for h in hidden_state)\n",
        "\n",
        "            avg_loss = total_loss / len(train_data_iter)\n",
        "            print(f\"Epoch {epoch + 1}/{n_epoch}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Step the learning rate scheduler\n",
        "            self.scheduler.step(avg_loss)\n",
        "\n",
        "    def next_word_probabilities(self, text_prefix: List[str]):\n",
        "        \"Return a list of probabilities for each word in the vocabulary.\"\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            ids_prefix = torch.tensor(\n",
        "                vocab.strs_to_ids(text_prefix), dtype=torch.long, device=self.device\n",
        "            ).view(-1, 1)  # (seq_len, batch_size=1)\n",
        "\n",
        "            # Initialize hidden state\n",
        "            hidden_state = None\n",
        "\n",
        "            # Forward pass\n",
        "            log_probs, hidden_state = self.network(ids_prefix, hidden_state)\n",
        "\n",
        "            # Get the last timestep's log probabilities\n",
        "            last_log_probs = log_probs[-1, 0, :]  # (vocab_size,)\n",
        "\n",
        "            # Convert to probabilities\n",
        "            probabilities = last_log_probs.exp().cpu().numpy()\n",
        "            return probabilities.tolist()\n",
        "\n",
        "    def dataset_perplexity(self, dataset: List[str], batch_size: int = 64, seq_len: int = 32):\n",
        "        \"Return perplexity as a float.\"\n",
        "        self.network.eval()\n",
        "        data_iterator = LstmDataIterator(vocab.strs_to_ids(dataset), batch_size, seq_len, self.device)\n",
        "        criterion = nn.NLLLoss(ignore_index=vocab.str_to_id[vocab.pad_tok], reduction=\"sum\")\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hidden_state = None\n",
        "            for i in range(len(data_iterator)):\n",
        "                inputs, targets = data_iterator[i]\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                log_probs, hidden_state = self.network(inputs, hidden_state)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(\n",
        "                    log_probs.view(-1, log_probs.size(2)),\n",
        "                    targets.view(-1),\n",
        "                )\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Count non-padding tokens\n",
        "                non_pad_mask = targets.view(-1) != vocab.str_to_id[vocab.pad_tok]\n",
        "                total_tokens += non_pad_mask.sum().item()\n",
        "\n",
        "                # Detach hidden state\n",
        "                hidden_state = tuple(h.detach() for h in hidden_state)\n",
        "\n",
        "        # Calculate perplexity\n",
        "        avg_loss = total_loss / total_tokens\n",
        "        perplexity = math.exp(avg_loss)\n",
        "        return perplexity\n"
      ],
      "metadata": {
        "id": "m9mHvs_vL4c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYQURodaUkdm"
      },
      "outputs": [],
      "source": [
        "## Feel free to copy your original LSTM solution down here to modify for your report if you'd like.\n",
        "# YOUR CODE [optionally] HERE\n",
        "##\n",
        "\n",
        "lstm_model = LSTMModel(device=\"cuda\")\n",
        "lstm_model.train()\n",
        "\n",
        "print('lstm validation perplexity:', lstm_model.dataset_perplexity(tok_validation_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_truncated_distribution(lstm_model, 'lstm_predictions.npy', short=False)"
      ],
      "metadata": {
        "id": "sOhCbfXX_IYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31t5WdalUsYi"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Upload a submission with the following files to Gradescope:\n",
        "* proj_1.ipynb (rename to match this exactly)\n",
        "* lstm_predictions.npy (this should also include all improvements from your exploration)\n",
        "* neural_trigram_predictions.npy\n",
        "* bigram_predictions.npy\n",
        "* report.pdf\n",
        "\n",
        "You can upload files individually or as part of a zip file, but if using a zip file be sure you are zipping the files directly and not a folder that contains them.\n",
        "\n",
        "Be sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.  Note that the test set perplexities shown by the autograder are on a completely different scale from your validation set perplexities due to truncating the distribution and selecting different text.  Don't worry if the values seem much worse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCf9AeaYUsqB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}